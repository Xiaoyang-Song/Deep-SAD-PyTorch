mkdir: cannot create directory ‘log/DeepSAD’: File exists
mkdir: cannot create directory ‘log/DeepSAD/fashionmnist’: File exists
INFO:root:Log file is ../log/DeepSAD/fashionmnist/log.txt
INFO:root:Data path is ../data
INFO:root:Export path is ../log/DeepSAD/fashionmnist
INFO:root:Dataset: fmnist
INFO:root:Normal class: [0, 1, 2, 3, 4, 5, 6, 7]
INFO:root:Ratio of labeled normal train samples: 0.00
INFO:root:Ratio of labeled anomalous samples: 0.01
INFO:root:Pollution ratio of unlabeled train data: 0.10
INFO:root:Known anomaly class: 1
INFO:root:Network: mnist_LeNet
INFO:root:Eta-parameter: 1.00
INFO:root:Set seed to 999.
INFO:root:Computation device: cuda
INFO:root:Number of threads: 0
INFO:root:Number of dataloader workers: 0
INFO:root:Pretraining: True
INFO:root:Pretraining optimizer: adam
INFO:root:Pretraining learning rate: 0.0001
INFO:root:Pretraining epochs: 150
INFO:root:Pretraining learning rate scheduler milestones: (0,)
INFO:root:Pretraining batch size: 128
INFO:root:Pretraining weight decay: 0.0005
INFO:root:Starting pretraining...
/home/xysong/.conda/envs/OoD/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/xysong/.conda/envs/OoD/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:  LR scheduler: new learning rate is 1e-05
INFO:root:| Epoch: 001/150 | Train Time: 6.746s | Train Loss: 0.172985 |
INFO:root:| Epoch: 002/150 | Train Time: 5.981s | Train Loss: 0.112058 |
INFO:root:| Epoch: 003/150 | Train Time: 6.066s | Train Loss: 0.081005 |
INFO:root:| Epoch: 004/150 | Train Time: 5.958s | Train Loss: 0.062001 |
INFO:root:| Epoch: 005/150 | Train Time: 5.963s | Train Loss: 0.048399 |
INFO:root:| Epoch: 006/150 | Train Time: 6.066s | Train Loss: 0.040062 |
INFO:root:| Epoch: 007/150 | Train Time: 5.966s | Train Loss: 0.034529 |
INFO:root:| Epoch: 008/150 | Train Time: 5.960s | Train Loss: 0.030592 |
INFO:root:| Epoch: 009/150 | Train Time: 5.986s | Train Loss: 0.027700 |
INFO:root:| Epoch: 010/150 | Train Time: 6.054s | Train Loss: 0.025555 |
INFO:root:| Epoch: 011/150 | Train Time: 5.956s | Train Loss: 0.023911 |
INFO:root:| Epoch: 012/150 | Train Time: 5.958s | Train Loss: 0.022636 |
INFO:root:| Epoch: 013/150 | Train Time: 5.982s | Train Loss: 0.021588 |
INFO:root:| Epoch: 014/150 | Train Time: 6.075s | Train Loss: 0.020681 |
INFO:root:| Epoch: 015/150 | Train Time: 5.988s | Train Loss: 0.019888 |
INFO:root:| Epoch: 016/150 | Train Time: 5.962s | Train Loss: 0.019164 |
INFO:root:| Epoch: 017/150 | Train Time: 5.967s | Train Loss: 0.018463 |
INFO:root:| Epoch: 018/150 | Train Time: 5.987s | Train Loss: 0.017726 |
